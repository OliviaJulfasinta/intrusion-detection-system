# -*- coding: utf-8 -*-
"""Intrusion Detection System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_VQwx-5Fdk9tlSw4A4DKWbcD9_658Ga_
"""

# Unduh dataset
!wget https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain%2B.txt
!wget https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTest%2B.txt

# Import library yang diperlukan
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Mengatur tampilan grafik
plt.style.use('ggplot')

# Fungsi untuk memuat dataset
def load_dataset(train_path, test_path):
    # Nama kolom dataset NSL-KDD
    columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',
               'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',
               'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',
               'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login',
               'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',
               'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',
               'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',
               'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',
               'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'class', 'difficulty']

    # Memuat dataset
    train_data = pd.read_csv(train_path, header=None, names=columns)
    test_data = pd.read_csv(test_path, header=None, names=columns)

    # Menghapus kolom difficulty yang tidak relevan
    train_data.drop('difficulty', axis=1, inplace=True)
    test_data.drop('difficulty', axis=1, inplace=True)

    return train_data, test_data

# untuk cek apakah dataset sudah dimuat dengan benar
train_data, test_data = load_dataset("KDDTrain+.txt", "KDDTest+.txt")
print("Bentuk data training:", train_data.shape)
print("Bentuk data testing:", test_data.shape)
print("\nContoh 5 baris pertama data training:")
print(train_data.head())

# untuk cek distribusi kelas pada data
print("\nDistribusi kelas pada data training:")
print(train_data['class'].value_counts())
print("\nDistribusi kelas pada data testing:")
print(test_data['class'].value_counts())

# Fungsi untuk preprocessing data
def preprocess_data(train_data, test_data):
    # Mengubah label menjadi biner (normal = 0, attack = 1)
    train_data['binary_class'] = train_data['class'].apply(lambda x: 0 if x == 'normal' else 1)
    test_data['binary_class'] = test_data['class'].apply(lambda x: 0 if x == 'normal' else 1)

    # Pisahkan fitur dan label
    X_train = train_data.drop(['class', 'binary_class'], axis=1)
    y_train = train_data['binary_class']
    X_test = test_data.drop(['class', 'binary_class'], axis=1)
    y_test = test_data['binary_class']

    # Identifikasi fitur kategorikal dan numerik
    categorical_cols = ['protocol_type', 'service', 'flag']
    numeric_cols = [col for col in X_train.columns if col not in categorical_cols]

    # One-hot encoding untuk fitur kategorikal

    encoder = OneHotEncoder(sparse_output=False)
    X_train_cat = encoder.fit_transform(X_train[categorical_cols])
    X_test_cat = encoder.transform(X_test[categorical_cols])

    # Normalisasi fitur numerik
    scaler = MinMaxScaler()
    X_train_num = scaler.fit_transform(X_train[numeric_cols])
    X_test_num = scaler.transform(X_test[numeric_cols])

    # Gabungkan fitur kategorikal dan numerik
    X_train_processed = np.hstack((X_train_num, X_train_cat))
    X_test_processed = np.hstack((X_test_num, X_test_cat))

    # Membagi data train menjadi train dan validation
    X_train_final, X_val, y_train_final, y_val = train_test_split(
        X_train_processed, y_train, test_size=0.15, random_state=42
    )

    return X_train_final, X_val, X_test_processed, y_train_final, y_val, y_test

# Lakukan preprocessing data
X_train, X_val, X_test, y_train, y_val, y_test = preprocess_data(train_data, test_data)

# untuk cek bentuk data setelah preprocessing
print("Bentuk data training:", X_train.shape)
print("Bentuk data validasi:", X_val.shape)
print("Bentuk data testing:", X_test.shape)

# untuk cek distribusi kelas setelah preprocessing
print("\nDistribusi kelas pada data training:")
print(pd.Series(y_train).value_counts())
print("\nDistribusi kelas pada data validasi:")
print(pd.Series(y_val).value_counts())
print("\nDistribusi kelas pada data testing:")
print(pd.Series(y_test).value_counts())

# Fungsi untuk membangun model DNN
def build_model(input_dim):
    model = Sequential([
        Dense(128, activation='relu', input_dim=input_dim),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(32, activation='relu'),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

    return model

# Bangun model
model = build_model(X_train.shape[1])

# Tampilkan ringkasan model
model.summary()

# Fungsi untuk melatih model
def train_model(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=64):
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=3,
        restore_best_weights=True
    )

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stopping],
        verbose=1
    )

    return model, history

# Latih model
model, history = train_model(model, X_train, y_train, X_val, y_val)

# Visualisasi hasil pelatihan
plt.figure(figsize=(12, 5))

# Plot akurasi
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# Fungsi untuk evaluasi model
def evaluate_model(model, X_test, y_test):
    # Prediksi probabilitas
    y_pred_prob = model.predict(X_test)

    # Konversi probabilitas ke label biner
    y_pred = (y_pred_prob > 0.5).astype(int).flatten()

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Hitung metrik evaluasi lainnya
    report = classification_report(y_test, y_pred)

    # Plot ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
    roc_auc = auc(fpr, tpr)

    # Visualisasi confusion matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

    # Visualisasi ROC curve
    plt.figure(figsize=(10, 8))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")
    plt.show()

    # Print classification report
    print("Classification Report:")
    print(report)

    return y_pred, y_pred_prob

# Evaluasi model
y_pred, y_pred_prob = evaluate_model(model, X_test, y_test)

# Simpan model
model.save('ids_model.h5')
print("Model berhasil disimpan sebagai 'ids_model.h5'")

# Unduh model ke komputer lokal
from google.colab import files
files.download('ids_model.h5')

# Tampilkan ringkasan proyek
print("RINGKASAN PROYEK DEEP LEARNING: SISTEM DETEKSI SERANGAN (IDS)")
print("="*70)
print("\n1. Latar Belakang Proyek:")
print("   - Sistem Deteksi Serangan (IDS) menggunakan deep learning untuk")
print("     mendeteksi aktivitas mencurigakan dalam jaringan komputer")
print("   - Menggunakan dataset NSL-KDD yang standar untuk IDS")
print("   - Klasifikasi biner: normal vs serangan")

print("\n2. Teknik Kecerdasan Buatan yang Digunakan:")
print("   - Deep Neural Networks (DNN) dengan beberapa hidden layer")
print("   - Feature Scaling dan Normalization")
print("   - One-Hot Encoding untuk fitur kategorikal")
print("   - Dropout Regularization untuk mencegah overfitting")
print("   - Early Stopping untuk mengoptimalkan pelatihan")

print("\n3. Bagaimana Data Diambil:")
print("   - Dataset NSL-KDD diunduh dari repository publik")
print("   - 41 fitur asli tentang koneksi jaringan")
print("   - Label disederhanakan menjadi kategori biner")
print("   - Data dibagi menjadi train, validation, dan test set")

print("\n4. Bagaimana Hasil Diolah:")
print("   - Evaluasi dengan metrik: Accuracy, Precision, Recall, F1-Score")
print("   - Visualisasi dengan Confusion Matrix dan ROC Curve")
print("   - Model mencapai akurasi yang baik dalam membedakan lalu lintas normal dan serangan")

print("\n5. Flow Diagram Proyek:")
print("   Pengumpulan Data → Preprocessing → Persiapan Dataset → Pelatihan Model → Prediksi → Evaluasi")